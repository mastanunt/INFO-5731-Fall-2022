{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/unt-iialab/INFO5731_Spring2020/blob/master/Assignments/INFO5731_Assignment_Four.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Four**\n",
    "\n",
    "In this assignment, you are required to conduct topic modeling, sentiment analysis based on **the dataset you created from assignment three**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Topic Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(30 points). This question is designed to help you develop a feel for the way topic modeling works, the connection to the human meanings of documents. Based on the dataset from assignment three, write a python program to **identify the top 10 topics in the dataset**. Before answering this question, please review the materials in lesson 8, especially the code for LDA, LSA, and BERTopic. The following information should be reported:\n",
    "\n",
    "(1) Features (text representation) used for topic modeling.\n",
    "\n",
    "(2) Top 10 clusters for topic modeling.\n",
    "\n",
    "(3) Summarize and describe the topic for each cluster. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.options.display.max_columns = 999\n",
    "df = pd.read_csv(\"datafile.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.iloc[:, 1]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuFPKhC0m1fd"
   },
   "outputs": [],
   "source": [
    "# for Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "# lemmatization\n",
    "import spacy\n",
    "# To Plot tools\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['perfect', 'everi', 'aspect']\n"
     ]
    }
   ],
   "source": [
    "#To convert sentences to words\n",
    "def sentences_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sent), deacc=True))  # deacc=True removes punctuations\n",
    "\n",
    "data_words = list(sentences_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 Building the bigram and trigram models for given\n",
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100)#high threshold\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "# making sentence fast and get a sentence joined as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "# trigram ex\n",
    "print(trigram_mod[bigram_mod[data_words[0]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions for stopwords, bigrams, trigrams  lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    t_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        t_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "    return t_out\n",
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Form Bigrams\n",
    "data_words_bigrams = make_bigrams(data_words_nostops)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(data_words_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
    "\n",
    "print(data_lemmatized[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word_words = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts_1 = data_lemmatized\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word_words.doc2bow(text) for text in texts_1]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, you can see a human-readable form of the corpus itself.\n",
    "# Human readable format of corpus (term-frequency)\n",
    "[[(id2word_words[id], freq) for id, freq in cp] for cp in corpus[:1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word_words,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)\n",
    "# Print the Keyword in the 10 topics\n",
    "from pprint import pprint\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "'''TOPIC 1: Regarding the videos showing the struggle that the victim was going through.\n",
    "TOPIC 2: About black people and their supremacy. \n",
    "TOPIC 3: Working and learning together to stop racial injustice. \n",
    "TOPIC 4: Regarding the polices being racists.\n",
    "TOPIC 5: Regarding the racial crimes.\n",
    "TOPIC 6: Regarding the pain and guilt felt by the families.\n",
    "TOPIC 7: Regarding the black lives matter movement. \n",
    "TOPIC 8: Regarding love, peace and support. \n",
    "TOPIC 9: Regarding racial injustice.\n",
    "TOPIC 10: Regarding justic for who were targeted for hate crime.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(30 points). Sentiment analysis also known as opinion mining is a sub field within Natural Language Processing (NLP) that builds machine learning algorithms to classify a text according to the sentimental polarities of opinions it contains, e.g., positive, negative, neutral. The purpose of this question is to develop a machine learning classifier for sentiment analysis. Based on the dataset from assignment three, write a python program to implement a sentiment classifier and evaluate its performance. Notice: **80% data for training and 20% data for testing**.  \n",
    "\n",
    "(1) Features used for sentiment classification and explain why you select these features.\n",
    "\n",
    "(2) Select two of the supervised learning algorithm from scikit-learn library: https://scikit-learn.org/stable/supervised_learning.html#supervised-learning, to build a sentiment classifier respectively. Note: Cross-validation (5-fold or 10-fold) should be conducted. Here is the reference of cross-validation: https://scikit-learn.org/stable/modules/cross_validation.html.\n",
    "\n",
    "(3) Compare the performance over accuracy, precision, recall, and F1 score for the two algorithms you selected. Here is the reference of how to calculate these metrics: https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vATjQNTY8buA"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "#1\n",
    "'''I utilized the TF-IDF vectorizer to extract features from my data corpus since it provides information on the relevance of the words in addition to their frequency, which I believe gives the words more weight when analyzing the various classes and enhancing the precision of our model.'''\n",
    "\n",
    "data = pd.read_csv(\"datafile.csv\")\n",
    "data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-IDF Vectorization\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "p = tfidf_vectorizer.fit_transform(data['clean_text'].apply(lambda p: np.str_(p)))\n",
    "q = data['sentiment']\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "p_train, p_test, q_train, q_test = train_test_split(p,q,test_size = 0.2, random_state = 202)\n",
    "\n",
    "print (\"p_train\", p_train.shape)\n",
    "print (\"p_test\", p_test.shape)\n",
    "print (\"q_train\", q_train.shape)\n",
    "print (\"q_test\", q_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(p_train,q_train)\n",
    "predictions_nb = nb.predict(p_test)\n",
    "predictions_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "c = classification_report(q_test, predictions_nb)\n",
    "print(\"Classification Report: \", \"\\n\", \"\\n\",c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies_nb = cross_val_score(estimator = nb, P = p_train, Q = q_train, cv = 10)\n",
    "\n",
    "print(f\" The Accuracy for Naive Bayes Model is :  {round(accuracies_nb.mean()*100)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#svm\n",
    "from sklearn import svm\n",
    "\n",
    "t = svm.SVC(kernel='linear')\n",
    "t.fit(p_train, q_train)\n",
    "predictions_svm = t.predict(p_test)\n",
    "predictions_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_svm = classification_report(q_test, predictions_svm)\n",
    "print(\"Classification Report: \", \"\\n\", \"\\n\",cr_svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies_svm = cross_val_score(estimator = t, P = p_train, Q = q_train, cv = 10)\n",
    "\n",
    "print(f\"Accuracy of the SVM Model is :  {round(accuracies_svm.mean()*100)}%\")\n",
    "#3\n",
    "'''The accuracy of the SVM model after 10-fold cross validation is 79% as opposed to 72% for the Naive Bayes model. This shows that the SVM model outperforms the naive bayes model when it comes to categorizing tweets into discrete categories.'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 3: House price prediction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(40 points). You are required to build a **regression** model to predict the house price with 79 explanatory variables describing (almost) every aspect of residential homes. The purpose of this question is to practice regression analysis, an supervised learning model. The training data, testing data, and data description files can be download from canvas. Here is an axample for implementation: https://towardsdatascience.com/linear-regression-in-python-predict-the-bay-areas-home-price-5c91c8378878. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"test.csv\")\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to divide the data into numerical, categorical.\n",
    "\n",
    "train_categorical_data = [var for var in train_data.columns if train_data[var].dtype=='O']\n",
    "train_numerical_data = [var for var in train_data.columns if train_data[var].dtype=='float']\n",
    "# imputing the missing values by the attributes mean for all the numerical attributes.\n",
    "\n",
    "for h in train_numerical_data:\n",
    "    train_data[h].fillna(value = train_data[h].mean(),inplace = True)\n",
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividing the data into numerical and categorical.\n",
    "\n",
    "test_categorical_data = [var for var in test_data.columns if test_data[var].dtype=='O']\n",
    "test_numerical_data = [var for var in test_data.columns if test_data[var].dtype=='float']\n",
    "# imputing the missing values by the attributes mean for all the numerical attributes.\n",
    "\n",
    "for h in test_numerical_data:\n",
    "    test_data[h].fillna(value = test_data[h].mean(),inplace = True)\n",
    "#EDA\n",
    "train_data.hist(bins=40, figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "att = [\"SalePrice\", \"OverallQual\", \"GrLivArea\", \"GarageCars\", \"GarageArea\",\"TotalBsmtSF\",\"1stFlrSF\", \"FullBath\",\"YearBuilt\" ]\n",
    "scatter_matrix(train_data[att], figsize=(25, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation analysis\n",
    "plt.figure(figsize = (25,20))\n",
    "sns.heatmap(train_data.corr(), annot = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmatrix = train_data.corr().abs()\n",
    "upper_tri = cmatrix.where(np.triu(np.ones(cmatrix.shape),k=1).astype(np.bool))\n",
    "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > 0.80)]\n",
    "print(\"The attributes that needs to be dropped are:\\n \",to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.drop(columns = to_drop, axis = 1, inplace = True)\n",
    "test_data.drop(columns = to_drop, axis = 1, inplace = True)\n",
    "# dropping id attributes\n",
    "train_data.drop(labels = ['Id'], axis = 1, inplace = True)\n",
    "test_data.drop(labels = ['Id'], axis = 1, inplace = True)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "for column in train_data.columns:\n",
    "    encoder = LabelEncoder()\n",
    "    encoder.fit(list(train_data[column].values))\n",
    "    train_data[column] = encoder.transform(list(train_data[column].values))\n",
    "for column in test_data.columns:\n",
    "    encoder_test = LabelEncoder()\n",
    "    encoder_test.fit(list(test_data[column].values))\n",
    "    test_data[column] = encoder_test.transform(list(test_data[column].values))\n",
    "p_train = train_data.iloc[:,:-1]\n",
    "q_train = train_data.iloc[:,-1]\n",
    "p_test = test_data.iloc[:,:]\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(p_train, q_train)\n",
    "predictions_lr = lin_reg.predict(p_test)\n",
    "print(\"R-Square value: \", lin_reg.score(p_train,q_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'Predicted House Price Values': predictions_lr})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
