{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **The fifth in-class-exercise (40 points in total, 11/17/2022)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text classification as well as the performance evaluation. In addition, you are requried to conduct *10 fold cross validation (https://scikit-learn.org/stable/modules/cross_validation.html)* in the training. \n",
    "\n",
    "The dataset can be download from canvas. The dataset contains two files train data and test data for sentiment analysis in IMDB review, it has two categories: 1 represents positive and 0 represents negative. You need to split the training data into training and validate data (80% for training and 20% for validation, https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6) and perform 10 fold cross validation while training the classifier. The final trained model was final evaluated on the test data. \n",
    "\n",
    "Algorithms:\n",
    "\n",
    "(1) MultinominalNB\n",
    "\n",
    "(2) SVM \n",
    "\n",
    "(3) KNN \n",
    "\n",
    "(4) Decision tree\n",
    "\n",
    "(5) Random Forest\n",
    "\n",
    "(6) XGBoost\n",
    "\n",
    "Evaluation measurement:\n",
    "\n",
    "(1) Accuracy\n",
    "\n",
    "(2) Recall\n",
    "\n",
    "(3) Precison \n",
    "\n",
    "(4) F-1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "# Saving the train data in a dataframe\n",
    "\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('/content/drive/MyDrive/exercise05_datacollection/exercise09_datacollection/stsa-train.txt',sep = 'delimiter=',header= None,names=['reviews'])\n",
    "train_data[['sentiment','reviews']] = train_data['reviews'].str.split(\" \", 1, expand=True)\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the test data in a dataframe\n",
    "\n",
    "test_data = pd.read_csv('/content/drive/MyDrive/exercise05_datacollection/exercise09_datacollection/stsa-test.txt',sep = 'delimiter=',header= None,names=['reviews'])\n",
    "test_data[['sentiment','reviews']] = test_data['reviews'].str.split(\" \", 1, expand=True)\n",
    "test_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries for data cleaning\n",
    "\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stopword=nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wl= WordNetLemmatizer()\n",
    "\n",
    "def clean(review):\n",
    "    review =\"\".join([word.lower() for word in review if word not in string.punctuation])\n",
    "    review = re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", review)\n",
    "    tokens = re.split('\\W+',review)\n",
    "    review = [wl.lemmatize(word) for word in tokens if word not in stopword]\n",
    "    return review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# Finding the frequency of the individual words in the reviews using tf-idf vectorizer\n",
    "tfidf_vect = TfidfVectorizer(analyzer = clean)\n",
    "X_tfidf = tfidf_vect.fit_transform(train_data['reviews'])\n",
    "\n",
    "print(X_tfidf.shape)\n",
    "\n",
    "\n",
    "# Saving both the train and test words in a different data frame \n",
    "X_tfidf_df=pd.DataFrame(X_tfidf.toarray())\n",
    "X_tfidf_df.columns=tfidf_vect.get_feature_names()\n",
    "X_test_tfidf = tfidf_vect.transform(test_data['reviews'])\n",
    "print(X_test_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "\n",
    "# Sampling the training set\n",
    "x_train, x_test, y_train, y_test = train_test_split(X_tfidf_df, train_data['sentiment'].values,\n",
    "                                                test_size=0.2, random_state=42)\n",
    "# Model fit using .fit()\n",
    "model_mnb = mnb.fit(x_train,y_train)\n",
    "y_pred_mnb = model_mnb.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_mnb,y_test))\n",
    "print(classification_report(y_test,y_pred_mnb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the cross-validation scores for MultinominalNB:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(mnb, x_test, y_test, cv=10)\n",
    "print(\"MultinominalNB score: \",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_validate, StratifiedKFold\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "svm = LinearSVC()\n",
    "model_svm = svm.fit(x_train,y_train)\n",
    "y_pred_svm = model_svm.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_svm,y_test))\n",
    "print(classification_report(y_test,y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the cross-validation scores for SVM:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(svm, x_test, y_test, cv=10)\n",
    "print(\"SVM score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=5,n_jobs=-1)\n",
    "model_knn = knn.fit(x_train,y_train)\n",
    "y_pred_knn = model_knn.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_knn,y_test))\n",
    "print(classification_report(y_test,y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cross-value score for KNN:\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(knn, x_test, y_test, cv=10)\n",
    "print(\"KNN score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "model_dt = dt.fit(x_train,y_train)\n",
    "y_pred_dt = model_dt.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_dt,y_test))\n",
    "print(classification_report(y_test,y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cross value score for Decision Tree:\n",
    "\n",
    "scores = cross_val_score(dt, x_test, y_test, cv=10)\n",
    "print(\"Decision tree score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "model_rf = rf.fit(x_train,y_train)\n",
    "y_pred_rf = model_rf.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_rf,y_test))\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cross value score for Random Forest:\n",
    "\n",
    "cores = cross_val_score(rf, x_test, y_test, cv=10)\n",
    "print(\"Random forest score\",scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "xgb = XGBClassifier()\n",
    "model_xgb = xgb.fit(x_train,y_train)\n",
    "y_pred_xgb = model_xgb.predict(x_test)\n",
    "print('Accuracy %s' % accuracy_score(y_pred_xgb,y_test))\n",
    "print(classification_report(y_test,y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating cross value score for XGBoost:\n",
    "\n",
    "scores = cross_val_score(xgb, x_test, y_test, cv=10)\n",
    "print(\"XGBoost score:\",scores.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(20 points) The purpose of the question is to practice different machine learning algorithms for text clustering Please downlad the dataset by using the following link. https://www.kaggle.com/PromptCloudHQ/amazon-reviews-unlocked-mobile-phones (You can also use different text data which you want)\n",
    "\n",
    "Apply the listed clustering methods to the dataset:\n",
    "\n",
    "K means, DBSCAN, Hierarchical clustering.\n",
    "\n",
    "You can refer to of the codes from the follwing link below. https://www.kaggle.com/karthik3890/text-clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your code here.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/MyDrive/Amazon_Unlocked_Mobile.csv/Amazon_Unlocked_Mobile.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(df.Rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition(x):\n",
    "    if x < 3:\n",
    "        return 'negative'\n",
    "    return 'positive'\n",
    "\n",
    "#changing reviews with Rating less than 3 to be positive\n",
    "actualScore = df['Rating']\n",
    "positiveNegative = actualScore.map(partition) \n",
    "df['RatingTransformed'] = positiveNegative\n",
    "\n",
    "sns.countplot(df.RatingTransformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "#initialising the snowball stemmer which is developed in recent years\n",
    "sno = nltk.stem.SnowballStemmer('english') \n",
    "stop=set(stopwords.words('english'))\n",
    "\n",
    "final = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanhtml(sentence): #function to clean the word of any html-tags\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, ' ', sentence)\n",
    "    return cleantext\n",
    "\n",
    "def cleanpunc(sentence):\n",
    "    cleaned_data = re.sub(r'[?|!|\\'|\"|#]',r'',sent)\n",
    "    cleaned_data = re.sub(r'[.|,|)|(|\\|/]',r' ',cleaned_data)\n",
    "    return  cleaned_data\n",
    "final = df.sample(n=1000, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "i=0\n",
    "str1=' '\n",
    "final_string=[]\n",
    "# store words from +ve reviews here\n",
    "all_positive_words=[] \n",
    "# store words from -ve reviews here.\n",
    "all_negative_words=[] \n",
    "s=''\n",
    "for sent in final['Reviews'].values:\n",
    "    filtered_sent=[]\n",
    "    #sent=cleanhtml(sent) # remove HTMl tags\n",
    "    try:\n",
    "        for w in sent.split():\n",
    "            for cleaned_words in cleanpunc(w).split():\n",
    "                if((cleaned_words.isalpha()) & (len(cleaned_words)>2)):    \n",
    "                    if(cleaned_words.lower() not in stop):\n",
    "                        s=(sno.stem(cleaned_words.lower())).encode('utf8')\n",
    "                        filtered_sent.append(s)\n",
    "                        if (final['RatingTransformed'].values)[i] == 'positive': \n",
    "                        #list of all words used to describe positive reviews\n",
    "                          all_positive_words.append(s) \n",
    "                        if(final['RatingTransformed'].values)[i] == 'negative':\n",
    "                        #list of all words used to describe negative reviews reviews\n",
    "                          all_negative_words.append(s) \n",
    "                    else:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue \n",
    "                   #final string of cleaned words\n",
    "        str1 = b\" \".join(filtered_sent)\n",
    "      \n",
    "      #print('str1:', str1)\n",
    "        final_string.append(str1)\n",
    "        i+=1\n",
    "        except AttributeError as e:\n",
    "      # No words to split\n",
    "        final_string.append('')\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final['CleanedText']=final_string \n",
    "final['CleanedText']=final['CleanedText'].str.decode(\"utf-8\")\n",
    "final = final.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "#bow = count_vect.fit_transform(final['CleanedText'].values)\n",
    "bow = count_vect.fit_transform(final['CleanedText'].values)\n",
    "print(bow.shape)\n",
    "terms = count_vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model = KMeans(n_clusters = 10,init='k-means++',random_state=99)\n",
    "model.fit(bow)\n",
    "\n",
    "labels = model.labels_\n",
    "cluster_center=model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "silhouette_score = metrics.silhouette_score(bow, labels, metric='euclidean')\n",
    "silhouette_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrme = final\n",
    "dafrme['Bow Clus Label'] = model.labels_ \n",
    "dafrme.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrme.groupby(['Bow Clus Label'])['Reviews'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = count_vect.get_feature_names()\n",
    "for i in range(10):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.bar([x for x in range(10)], dafrme.groupby(['Bow Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
    "plt.title('KMeans cluster points')\n",
    "plt.xlabel(\"Cluster number\")\n",
    "plt.ylabel(\"Number of points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vect = TfidfVectorizer()\n",
    "tf_idf = tf_idf_vect.fit_transform(final['CleanedText'].values)\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model_tf = KMeans(n_clusters = 10,random_state=99)\n",
    "model_tf.fit(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_tf = model_tf.labels_\n",
    "cluster_center_tf=model_tf.cluster_centers_\n",
    "cluster_center_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms1 = tf_idf_vect.get_feature_names()\n",
    "terms1[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "silhouette_score_tf = metrics.silhouette_score(tf_idf, labels_tf, metric='euclidean')\n",
    "silhouette_score_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrme1 = dafrme\n",
    "dafrme1['Tfidf Clus Label'] = model_tf.labels_\n",
    "dafrme1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrme1.groupby(['Tfidf Clus Label'])['Reviews'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Top terms per cluster:\")\n",
    "order_centroids = model_tf.cluster_centers_.argsort()[:, ::-1]\n",
    "for i in range(10):\n",
    "    print(\"Cluster %d:\" % i, end='')\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms1[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar([x for x in range(10)], dafrme1.groupby(['Tfidf Clus Label'])['Reviews'].count(), alpha = 0.4)\n",
    "plt.title('TFID cluster points')\n",
    "plt.xlabel(\"Cluster number\")\n",
    "plt.ylabel(\"Number of points\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average Word to Vector\n",
    "i=0\n",
    "list_of_sent=[]\n",
    "for sent in final['CleanedText'].values:\n",
    "    list_of_sent.append(sent.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "list_of_sent_train=[]\n",
    "for sent in final['CleanedText'].values:\n",
    "    filtered_sentence=[]\n",
    "    for w in sent.split():\n",
    "        if(len(filtered_sentence) > 10000):\n",
    "        continue\n",
    "        for cleaned_words in cleanpunc(w).split():\n",
    "        if(cleaned_words.isalpha()):\n",
    "            filtered_sentence.append(cleaned_words.lower())\n",
    "        else:\n",
    "            continue \n",
    "    list_of_sent_train.append(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_sent_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "w2v_model=gensim.models.Word2Vec(list_of_sent_train,size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "sent_vectors = []; # the avg-w2v for each sentence/review is stored in this train\n",
    "for sent in list_of_sent_train: # for each review/sentence\n",
    "    sent_vec = np.zeros(100) # as word vectors are of zero length\n",
    "    cnt_words =0; # num of words with a valid vector in the sentence/review\n",
    "    for word in sent: # for each word in a review/sentence\n",
    "        try:\n",
    "            vec = w2v_model.wv[word]\n",
    "            sent_vec += vec\n",
    "            cnt_words += 1\n",
    "        except:\n",
    "            pass\n",
    "    sent_vec /= cnt_words\n",
    "    sent_vectors.append(sent_vec)\n",
    "sent_vectors = np.array(sent_vectors)\n",
    "sent_vectors = np.nan_to_num(sent_vectors)\n",
    "sent_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means Clustering\n",
    "num_clus = [x for x in range(3,11)]\n",
    "num_clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing the best cluster using Elbow Method.\n",
    "# source credit,few parts of min squred loss info is taken from different parts of the stakoverflow answers.\n",
    "# this is used to understand to find the optimal clusters in differen way rather than used in BOW, TFIDF\n",
    "squared_errors = []\n",
    "for cluster in num_clus:\n",
    "  # Train Cluster\n",
    "    kmeans = KMeans(n_clusters = cluster).fit(sent_vectors) \n",
    "    # Appending the squared loss obtained in the list\n",
    "    squared_errors.append(kmeans.inertia_) \n",
    "# As argmin return the index of minimum loss.    \n",
    "optimal_clusters = np.argmin(squared_errors) + 2  \n",
    "plt.plot(num_clus, squared_errors)\n",
    "plt.title(\"Elbow Curve to find the no. of clusters.\")\n",
    "plt.xlabel(\"Number of clusters.\")\n",
    "plt.ylabel(\"Squared Loss.\")\n",
    "xy = (optimal_clusters, min(squared_errors))\n",
    "plt.annotate('(%s, %s)' % xy, xy = xy, textcoords='data')\n",
    "plt.show()\n",
    "\n",
    "print (\"The optimal number of clusters obtained is - \", optimal_clusters)\n",
    "print (\"The loss for optimal cluster is - \", min(squared_errors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "model2 = KMeans(n_clusters = optimal_clusters)\n",
    "model2.fit(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cluster_pred=model2.predict(sent_vectors)\n",
    "word_cluster_pred_2=model2.labels_\n",
    "word_cluster_center=model2.cluster_centers_\n",
    "word_cluster_center[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrmea = dafrme1\n",
    "dafrmea['AVG-W2V Clus Label'] = model2.labels_\n",
    "dafrmea.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dafrmea.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DBSCAN \n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Computing 200th Nearest neighbour distance\n",
    "min_Pts = 2 * 100\n",
    "# The function returns the no in the array which is  greater than or equal to itself.\n",
    "def lower_bound(nums, target): \n",
    "    l, r = 0, len(nums) - 1\n",
    "    # Binary searching.\n",
    "    while l <= r: \n",
    "        mid_term = int(l + (r - l) / 2)\n",
    "        if nums[mid_term] >= target:\n",
    "            r = mid_term - 1\n",
    "        else:\n",
    "            l = mid_term + 1\n",
    "    return l\n",
    " # Returns the dist of 200th nearest neighbour.\n",
    "def compute200thnearestneighbour(x, data):\n",
    "    dists = []\n",
    "    for val in data:\n",
    "      # computing distances.\n",
    "        dist = np.sum((x - val) **2 ) \n",
    "        # If dist is more than current largest distance found.\n",
    "        if(len(dists) == 200 and dists[199] > dist): \n",
    "          # Using the lower bound function to get the right position.\n",
    "            l = int(lower_bound(dists, dist)) \n",
    "            if l < 200 and l >= 0 and dists[l] > dist:\n",
    "                dists[l] = dist\n",
    "        else:\n",
    "            dists.append(dist)\n",
    "            dists.sort()\n",
    "    \n",
    "    return dists[199]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twohundrethneigh = []\n",
    "for val in sent_vectors[:1500]:\n",
    "    twohundrethneigh.append( compute200thnearestneighbour(val, sent_vectors[:1500]) )\n",
    "twohundrethneigh.sort()\n",
    "plt.figure(figsize=(14,4))\n",
    "plt.title(\"Elbow Method for Finding the right Eps hyperparameter\")\n",
    "plt.plot([x for x in range(len(twohundrethneigh))], twohundrethneigh)\n",
    "plt.xlabel(\"Number of points\")\n",
    "plt.ylabel(\"Distance of 200th Nearest Neighbour\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training DBSCAN :\n",
    "minPts = 2 * 100\n",
    "model = DBSCAN(eps = 5, min_samples = minPts, n_jobs=-1)\n",
    "model.fit(sent_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb = dafrmea\n",
    "dfdb['AVG-W2V Clus Label'] = model.labels_\n",
    "dfdb.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdb.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.cluster import hierarchy\n",
    "dendro=hierarchy.dendrogram(hierarchy.linkage(sent_vectors,method='ward'))\n",
    "plt.axhline(y=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=5, affinity='euclidean', linkage='ward')  #took n=5 from dendrogram curve \n",
    "Agg=cluster.fit_predict(sent_vectors)\n",
    "aggdfa = dfdb\n",
    "aggdfa['AVG-W2V Clus Label'] = cluster.labels_\n",
    "aggdfa.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggdfa.groupby(['AVG-W2V Clus Label'])['Reviews'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In one paragraph, please compare K means, DBSCAN and Hierarchical clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#You can write you answer here. (No code needed)\n",
    "\n",
    "K-means clustering is based on the distance between the data points and the centroids of the clusters themselves. DBSCAN is used to perform clustering that is based on density. Here the areas where the points are concetrated the most are found and are segregated from empty space. Hierarchical clustering, as the name suggests involves clustering in layers. It begins with considering each data point as a seperate cluster and then finds the two clusters that are closer to each other. In machine learning, clustering techniques like as K-means and DBScan are widely used. In K-means clusters must have the same feature size when compared to DBScan it may not have the same feature size. K-means clusters are sensitive to the specified number of clusters. In DBScan no need to specify the number of clusters.K-means clustering is efficient for large data sets but DBScan can not handle high dimensional datasets.K-means has one parameter which is the number of clusters and DBScan has two parameters which are radius and minimum points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
