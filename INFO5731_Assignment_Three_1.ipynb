{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Muppasaisrija/Muppa_NFO5731_-SPRING2022/blob/main/INFO5731_Assignment_Three_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USSdXHuqnwv9"
   },
   "source": [
    "# **INFO5731 Assignment Three**\n",
    "\n",
    "In this assignment, you are required to conduct information extraction, semantic analysis based on **the dataset you collected from assignment two**. You may use scipy and numpy package in this assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWxodXh5n4xF"
   },
   "source": [
    "# **Question 1: Understand N-gram**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TenBkDJ5n95k"
   },
   "source": [
    "(45 points). Write a python program to conduct N-gram analysis based on the dataset in your assignment two:\n",
    "\n",
    "(1) Count the frequency of all the N-grams (N=3).\n",
    "\n",
    "(2) Calculate the probabilities for all the bigrams in the dataset by using the fomular count(w2 w1) / count(w2). For example, count(really like) / count(really) = 1 / 3 = 0.33.\n",
    "\n",
    "(3) Extract all the **noun phrases** and calculate the relative probabilities of each review in terms of other reviews (abstracts, or tweets) by using the fomular frequency (noun phrase) / max frequency (noun phrase) on the whole dataset. Print out the result in a table with column name the all the noun phrases and row name as all the 100 reviews (abstracts, or tweets). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "from nltk.corpus import stopwords\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fzEt1Uhn7H0j",
    "outputId": "aaaf5ded-5ed5-4a3d-e3d9-c00cca3b9033"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QX_4TGQC8hsx",
    "outputId": "9707534d-e6b1-4f41-d123-c4d01c72a7d6"
   },
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def get_lemmatized_docs(X):\n",
    "    stemmer = WordNetLemmatizer()\n",
    "    eng_stop_word_list = stopwords.words(\"english\")\n",
    "    bag_of_words = []\n",
    "    for sen in range(0, len(X)):\n",
    "    # removing all the special characters\n",
    "        document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "\n",
    "      # removing all single characters\n",
    "        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "\n",
    "      # removing multiple spaces and adding single space in place\n",
    "        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "\n",
    "      # removing the 'b' prefix added during byte conversion\n",
    "        document = re.sub(r'^b\\s+', '', document)\n",
    "\n",
    "      # removing numbers\n",
    "        document = re.sub(r'\\d+','',document)\n",
    "\n",
    "      # converting all words to lowercase\n",
    "        document = document.lower()\n",
    "      \n",
    "      # Splitting the cleaned document into a list of words, which will be subsequently added to a bag after more processing\n",
    "        document = document.split()\n",
    "        document = [stemmer.lemmatize(word) for word in document]\n",
    "        document = [word for word in document if word not in eng_stop_word_list]\n",
    "        document = ' '.join(document)\n",
    "\n",
    "        document = \" \".join(word for word in nltk.wordpunct_tokenize(document) if word.lower() in words or not word.isalpha())\n",
    "\n",
    "      #Bag of words\n",
    "        bag_of_words.append(document)\n",
    "\n",
    "    return bag_of_words\n",
    "\n",
    "df = pd.read_csv('/content/drive/MyDrive/Nokia Reviews.csv')\n",
    "#df.dropna(how='any', inplace=True)\n",
    "clean_docs  = get_lemmatized_docs(df[\"Review\"])\n",
    "print(len(clean_docs))\n",
    "print(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IxCZ-O-VEN9m",
    "outputId": "70fea76a-98a5-46e5-b2b3-b0af5b8c05c7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = [word for word in clean_docs if str(word) != 'nan' and str(word) != '']\n",
    "print(words)\n",
    "word_list = []\n",
    "for word in words:\n",
    "  [word_list.append(val) for val in word.split(' ')]\n",
    "\n",
    "word_str = ''\n",
    "for word in word_list:\n",
    "  word_str += word + ' '\n",
    "print(word_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0X2CKPg8Gl4_",
    "outputId": "64b749dc-3c2b-410a-bc4a-f32f574bd911"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "token_list = nltk.word_tokenize(word_str)\n",
    "res_trigrams = nltk.trigrams(token_list)\n",
    "freq_trigrams = nltk.FreqDist(res_trigrams)\n",
    "freq_trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "import collections\n",
    "token_bigram = nltk.word_tokenize(word_str)\n",
    "bigrams = ngrams(token_bigram,2)\n",
    "bigram_dist = nltk.FreqDist(bigrams)\n",
    "single_word_dist = nltk.FreqDist([word for word in word_str.split(' ')])\n",
    "\n",
    "for w1,w2 in bigram_dist:\n",
    "    count_w1_w2 = bigram_dist[(w1,w2)]\n",
    "    count_w1 = single_word_dist[w1]\n",
    "    if(count_w1 != 0):\n",
    "        print(w1,w2,(count_w1_w2/count_w1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import AnyStr\n",
    "\n",
    "from collections import Counter      \n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('/content/drive/MyDrive/Nokia Reviews.csv')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "lower_case = word_str.lower()\n",
    "word_tokens = nltk.word_tokenize(lower_case)\n",
    "parts_of_speech_tags = nltk.pos_tag(word_tokens)\n",
    "\n",
    "df.dropna(how='any', inplace=True)\n",
    "cnts = Counter( tag for word,  tag in parts_of_speech_tags)\n",
    "max_cnt = cnts['NN']\n",
    "df['word groups'] = pos_tag_sents(df['Review'].apply(word_tokenize).tolist())\n",
    "\n",
    "\n",
    "word_grps = df['word groups']\n",
    "first = []\n",
    "scnd = []\n",
    "for grp in word_grps:\n",
    "    cnt = Counter( tag for word,  tag in grp)\n",
    "    first.append(cnt['NN'])\n",
    "for val in first:\n",
    "    rel_prob = val/max_cnt\n",
    "    scnd.append(rel_prob)\n",
    "print(scnd)\n",
    "\n",
    "df1 = pd.DataFrame(first,columns = ['noun_count'])\n",
    "df2 = pd. DataFrame(scnd,columns = ['prob_noun'])\n",
    "res = pd.concat([df1,df2], axis=1, sort=False)\n",
    "res_frame = pd.concat([df,res], axis = 1, sort = False)\n",
    "res_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfpMRCrRwN6Z"
   },
   "source": [
    "# **Question 2: Undersand TF-IDF and Document representation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1dCQEbDawWCw"
   },
   "source": [
    "(20 points). Starting from the documents (all the reviews, or abstracts, or tweets) collected for assignment two, write a python program: \n",
    "\n",
    "(1) To build the **documents-terms weights (tf*idf) matrix bold text**.\n",
    "\n",
    "(2) To rank the documents with respect to query (design a query by yourself, for example, \"An Outstanding movie with a haunting performance and best character development\") by using **cosine similarity**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vATjQNTY8buA",
    "outputId": "e82240a1-afcd-4d2b-9166-a8047824b7b2"
   },
   "outputs": [],
   "source": [
    "# Write your code here\\\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "text = []\n",
    "for x in df['Review']:\n",
    "    text.append(x)\n",
    "vectorizer = TfidfVectorizer()\n",
    "tf_idf_matrix = vectorizer.fit_transform(text)\n",
    "attributes = vectorizer.get_feature_names()\n",
    "index=[n for n in range(1, len(text)+1)]\n",
    "vectorizer = TfidfVectorizer(input=text, min_df=0.1, stop_words='english')\n",
    "tf_idf=pd.DataFrame(tf_idf_matrix.T.todense(), index=attributes, columns=index)\n",
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Take these reviews with a pinch of salt.\"\n",
    "cleaned_query = get_lemmatized_docs([query])\n",
    "print(cleaned_query[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cosine similarity\n",
    "from nltk import corpus\n",
    "import pandas\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "text = []\n",
    "for x in df['Review']:\n",
    "    text.append(x)\n",
    "\n",
    "text.append(cleaned_query[0])\n",
    "vectorizer = TfidfVectorizer(input=text, min_df=0.1, stop_words='english')\n",
    "matrix = vectorizer.fit_transform(text)\n",
    "\n",
    "cs_dict = {}\n",
    "for i in range(0, matrix.shape[0]):\n",
    "    sim=cosine_similarity(matrix[80], matrix[i])\n",
    "    cs_dict[str(i)] = sim[0][0]\n",
    "    i+=1\n",
    "\n",
    "sorted_keys = (sorted(cs_dict, key=cs_dict.get,reverse=True))\n",
    "sorted_cosine_dict = {}\n",
    "for key in sorted_keys:\n",
    "    sorted_cosine_dict[key] = cs_dict[key];\n",
    "\n",
    "Review = []\n",
    "cos = []\n",
    "for y in sorted_cosine_dict:\n",
    "    text.append([int(y)])\n",
    "    cos.append(sorted_cosine_dict[y])\n",
    "dafrme1 = pd.DataFrame(Review)\n",
    "\n",
    "dafrme1['Rank'] = [i for i in range(1, len(sorted_keys)+1)]\n",
    "dafrme1['Cossim'] = cos\n",
    "dafrme1.head(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezcYP0ojXq5r"
   },
   "source": [
    "# **Question 3: Create your own word embedding model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkUkEJdqXq5s"
   },
   "source": [
    "(20 points). Use the data you collected for assignment two to build a word embedding model: \n",
    "\n",
    "(1) Train a 300-dimension word embedding (it can be word2vec, glove, ulmfit, bert, or others).\n",
    "\n",
    "(2) Visualize the word embedding model you created.\n",
    "\n",
    "Reference: https://machinelearningmastery.com/develop-word-embeddings-python-gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "7xnieW2ZXq5u",
    "outputId": "cc55fafc-0aa5-4faa-c88e-56b7db90a124"
   },
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "\n",
    "!pip install --upgrade gensim\n",
    "\n",
    "import gensim\n",
    "from gensim import models\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [word.split() for word in clean_docs if str(word) != 'nan' and str(word) != '']\n",
    "model = models.Word2Vec(docs,vector_size=300)\n",
    "words = list(model.wv.index_to_key)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.save_word2vec_format('model.bin')\n",
    "#model.wv.save_word2vec_format('model.txt', binary=False)\n",
    "model.save('model.bin')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = model.wv[model.wv.index_to_key]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(X)\n",
    "# create a scatter plot of the projection\n",
    "pyplot.scatter(result[:, 0], result[:, 1])\n",
    "words = list(model.wv.index_to_key)\n",
    "for i, word in enumerate(words):\n",
    "\tpyplot.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5mmYIfN8eYV"
   },
   "source": [
    "# **Question 4: Create your own training and evaluation data for sentiment analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wG1okfpH-mEd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsi2y4z88ngX"
   },
   "source": [
    "(15 points). **You dodn't need to write program for this question!** Read each review (abstract or tweet) you collected in detail, and annotate each review with a sentiment (positive, negative, or neutral). Save the annotated dataset into a csv file with three columns (first column: document_id, clean_text, sentiment), upload the csv file to GitHub and submit the file link blew. This datset will be used for assignment four: sentiment analysis and text classification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfvMKJjIXS5G"
   },
   "outputs": [],
   "source": [
    "# The GitHub link of your final csv file\n",
    "\n",
    "https://github.com/smmohua/INFO-5731/blob/main/tweet_sentiment_analysis.csv\n",
    "\n",
    "# Link: \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "INFO5731_Assignment_Three_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
